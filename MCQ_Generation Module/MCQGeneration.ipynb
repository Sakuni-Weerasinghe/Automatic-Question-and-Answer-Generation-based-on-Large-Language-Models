{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4kKh978vDA6zUSoBInvZJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakuni-Weerasinghe/Automatic-Question-and-Answer-Generation-based-on-Large-Language-Models/blob/master/MCQGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install sentencepiece\n",
        "!pip install pytorch-lightning\n",
        "!pip install sense2vec"
      ],
      "metadata": {
        "id": "dTicpG-FF9hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "HT9jfr3rwlk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "import tqdm.notebook as tq\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from transformers import T5Tokenizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import Trainer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from typing import Tuple\n",
        "from typing import List\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import toolz\n",
        "import time\n",
        "# from platform import dist\n",
        "import string\n",
        "import re\n",
        "import textwrap\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from sense2vec import Sense2Vec\n",
        "from collections import OrderedDict\n",
        "from typing import List\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast as T5Tokenizer\n",
        "    )"
      ],
      "metadata": {
        "id": "3dKeyQKzGEJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modules"
      ],
      "metadata": {
        "id": "Twu-aaddRD30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Duplicate Romveal"
      ],
      "metadata": {
        "id": "EtuLAiSJVpfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(items: List[str]) -> List[str]:\n",
        "    unique_items = []\n",
        "    normalized_unique_items = []\n",
        "\n",
        "    for item in items:\n",
        "        normalized_item = _normalize_item(item)\n",
        "\n",
        "        if normalized_item not in normalized_unique_items:\n",
        "            unique_items.append(item)\n",
        "            normalized_unique_items.append(normalized_item)\n",
        "\n",
        "    return unique_items\n",
        "\n",
        "def remove_distractors_duplicate_with_correct_answer(correct: str, distractors: List[str]) -> List[str]:\n",
        "    for distractor in distractors:\n",
        "        if _normalize_item(correct) == _normalize_item(distractor):\n",
        "            distractors.remove(distractor)\n",
        "\n",
        "    return distractors\n",
        "\n",
        "def _get_most_distinct_from_key(key: str, items: List[str]) -> List[str]:\n",
        "    #TODO: This seems not as useful. For example \"the family Phascolarctidae\" and \"the family Vombatidae\" are close, but good distractors.\n",
        "\n",
        "    return items\n",
        "\n",
        "def _get_most_distinct_from_each_other():\n",
        "    #TODO\n",
        "    # calculate bleu for each with each.\n",
        "    # find the most similar pair\n",
        "    # remove the second in the original list (assuming the list comes ordered by better)\n",
        "    # run until you get the desired amount\n",
        "    pass\n",
        "\n",
        "def _normalize_item(item) -> str:\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(item))))\n",
        "\n",
        "def _calculate_nltk_bleu(references: List[str], hypothesis: str, bleu_n: int = 1):\n",
        "    if hypothesis == '':\n",
        "        return 0, 0, 0, 0\n",
        "\n",
        "    # Word tokenize\n",
        "    refs_tokenized = list(map(lambda x: word_tokenize(x), references))\n",
        "    hyp_tokenized = word_tokenize(hypothesis)\n",
        "\n",
        "    # Smoothing function to avoid the cases where it resuts 1.0 in the cases when // Corpus/Sentence contains 0 counts of 2-gram overlaps. BLEU scores might be undesirable; use SmoothingFunction() //\n",
        "    chencherry = SmoothingFunction()\n",
        "    bleu = 0\n",
        "\n",
        "    if bleu_n == 1:\n",
        "        bleu = sentence_bleu(refs_tokenized, hyp_tokenized, weights=(1, 0, 0, 0), smoothing_function=chencherry.method2)\n",
        "    elif bleu_n == 2:\n",
        "        bleu = sentence_bleu(refs_tokenized, hyp_tokenized, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method2)\n",
        "    elif bleu_n == 3:\n",
        "        bleu = sentence_bleu(refs_tokenized, hyp_tokenized, weights=(0.33, 0.33, 0.33, 0), smoothing_function=chencherry.method2)\n",
        "    elif bleu_n == 4:\n",
        "        bleu = sentence_bleu(refs_tokenized, hyp_tokenized, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method2)\n",
        "\n",
        "    return bleu"
      ],
      "metadata": {
        "id": "9BypO2XzRHNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text Cleaning"
      ],
      "metadata": {
        "id": "HUvo_ETnV7TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean the text from symbols and additional information.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text.\n",
        "\n",
        "    Returns:\n",
        "        str: CLeaned text.\n",
        "    \"\"\"\n",
        "    cleaned_text = _remove_brackets(text)\n",
        "    cleaned_text = _remove_square_brackets(cleaned_text)\n",
        "    cleaned_text = _remove_multiple_spaces(cleaned_text)\n",
        "    cleaned_text = _replace_weird_hyphen(cleaned_text)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "def _remove_brackets(text: str) -> str:\n",
        "    \"\"\" Remove brackets '(', ')' and the information between them.\n",
        "\n",
        "    e.g. \"The koala has a body length of 60–85 cm (24–33 in).\"\n",
        "\n",
        "    Args:\n",
        "        text (str): The text.\n",
        "\n",
        "    Returns:\n",
        "        str: CLeaned text.\n",
        "    \"\"\"\n",
        "    return re.sub(r'\\((.*?)\\)', lambda L: '', text)\n",
        "\n",
        "\n",
        "def _remove_square_brackets(text: str) -> str:\n",
        "    \"\"\" Remove square brackets '[', ']' and the information between them.\n",
        "\n",
        "    e.g. The koala[1] is cool.\"\n",
        "\n",
        "    Args:\n",
        "        text (str): The text.\n",
        "\n",
        "    Returns:\n",
        "        str: CLeaned text.\n",
        "    \"\"\"\n",
        "\n",
        "    return re.sub(r'\\[(.*?)\\]', lambda L: '', text)\n",
        "\n",
        "\n",
        "def _remove_multiple_spaces(text: str) -> str:\n",
        "    \"\"\"Remove multiple white spaces.\n",
        "\n",
        "    e.g. \"The koala         is     angry  !\"\n",
        "\n",
        "    Args:\n",
        "        text (str): The text.\n",
        "\n",
        "    Returns:\n",
        "        str: CLeaned text.\n",
        "    \"\"\"\n",
        "\n",
        "    return re.sub(' +', ' ', text)\n",
        "\n",
        "\n",
        "def _replace_weird_hyphen(text: str) -> str:\n",
        "    \"\"\" Replace weird '–' hyphen that's not recognized as a delimeter by spacy.\n",
        "\n",
        "    e.g. '4–15 kg' -> '4-15 kg'\n",
        "    (You may not see a difference, but there fucking is. This motherfucker '–' is not recognized by spacy as a delimeter.)\n",
        "\n",
        "    Args:\n",
        "        text (str): The text.\n",
        "\n",
        "    Returns:\n",
        "        str: CLeaned text.\n",
        "    \"\"\"\n",
        "    return text.replace('–', '-')"
      ],
      "metadata": {
        "id": "z3XRpDfjV9f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Datasets"
      ],
      "metadata": {
        "id": "IIaQ_Tk6wv_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/file/d/1ggZqb_jCmq12gwmfBBQhOWUV4dN00S-o/view?usp=sharing\n",
        "#https://drive.google.com/file/d/1VrBghMWob_-mTz0sB5B8KLkq0MgdRrC9/view?usp=sharing\n",
        "#https://drive.google.com/file/d/192ibBUshejWnZm3vepWNMi-9kn5GypiD/view?usp=sharing\n",
        "\n",
        "!gdown --id 1ggZqb_jCmq12gwmfBBQhOWUV4dN00S-o #train.csv\n",
        "!gdown --id 1VrBghMWob_-mTz0sB5B8KLkq0MgdRrC9 #test.csv\n",
        "!gdown --id 192ibBUshejWnZm3vepWNMi-9kn5GypiD #val.csv"
      ],
      "metadata": {
        "id": "5jtVU-wfGH8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_csv('train.csv')\n",
        "test_dataset = pd.read_csv('test.csv')\n",
        "val_dataset = pd.read_csv('val.csv')"
      ],
      "metadata": {
        "id": "hPeYRlg8w6eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.shape,'train_dataset')\n",
        "print(test_dataset.shape, 'test_dataset')\n",
        "print(val_dataset.shape, 'val_dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd7F_3zpw9O9",
        "outputId": "2bd97cc6-6880-4d09-aa9f-d3e06e57aebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(606, 9) train_dataset\n",
            "(190, 9) test_dataset\n",
            "(152, 9) val_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_1 = train_dataset.copy()\n",
        "test_dataset_1 = test_dataset.copy()\n",
        "val_dataset_1 = val_dataset.copy()\n",
        "\n",
        "train_dataset_1 = train_dataset_1.dropna() #removing rows with missing values in the DataFrame\n",
        "test_dataset_1 = test_dataset_1.dropna() #removing rows with missing values in the DataFrame\n",
        "val_dataset_1 = val_dataset_1.dropna() #removing rows with missing values in the DataFrame\n",
        "\n",
        "# train_dataset_1.drop(columns=['option1', 'option2','option3','option4'], inplace=True) #answer_start and answer_end are not needed and are for the paragraph\n",
        "# test_dataset_1.drop(columns=['option1', 'option2','option3','option4'], inplace=True)\n",
        "# val_dataset_1.drop(columns=['option1', 'option2','option3','option4'], inplace=True)\n",
        "\n",
        "print(train_dataset_1.shape,'train_dataset_1')\n",
        "print(test_dataset_1.shape, 'test_dataset_1')\n",
        "print(val_dataset_1.shape, 'val_dataset_1')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSXgKY8dxEkK",
        "outputId": "ad22e8db-0d2b-472f-98f4-6fe6fab69006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(586, 9) train_dataset_1\n",
            "(185, 9) test_dataset_1\n",
            "(150, 9) val_dataset_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "MODEL_NAME = 't5-base'\n",
        "LEARNING_RATE = 0.0001\n",
        "SOURCE_MAX_TOKEN_LEN = 200\n",
        "TARGET_MAX_TOKEN_LEN = 80\n",
        "SEP_TOKEN = '<sep>'\n",
        "TOKENIZER_LEN = 32101  # after adding the new <sep> token\n"
      ],
      "metadata": {
        "id": "Pujr_j7zxSiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgCzk7uH6_Wa",
        "outputId": "97138fef-2e96-44c9-93d1-6deaea3eeed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Generation Model"
      ],
      "metadata": {
        "id": "dUlpI89x_gXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QGModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load tokenizer and model\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "        self.tokenizer.add_tokens([SEP_TOKEN])\n",
        "\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "        self.model.resize_token_embeddings(TOKENIZER_LEN)\n",
        "\n",
        "        # Resize the token embeddings to match the new tokenizer length\n",
        "        self.model.config.max_length = SOURCE_MAX_TOKEN_LEN\n",
        "        self.model.config.max_length_for_sum = TARGET_MAX_TOKEN_LEN\n",
        "\n",
        "        # Set learning rate\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "        self.log('test_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return self.optimizer"
      ],
      "metadata": {
        "id": "FhvdXjDB_qc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lmfwKLItzld",
        "outputId": "b8d4d01c-7252-46f8-d2a2-860634f40a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/file/d/1Vr1dapnNme4IqSJe6cOafutFSe3kBxqU/view?usp=sharing\n",
        "!gdown --id 1Vr1dapnNme4IqSJe6cOafutFSe3kBxqU"
      ],
      "metadata": {
        "id": "4zWgYlETGMsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuestionGenerator():\n",
        "    def __init__(self):\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "        # print('tokenizer len before: ', len(self.tokenizer))\n",
        "        self.tokenizer.add_tokens(SEP_TOKEN)\n",
        "        # print('tokenizer len after: ', len(self.tokenizer))\n",
        "        self.tokenizer_len = len(self.tokenizer)\n",
        "\n",
        "        checkpoint_path = 'best-checkpoint-v7.ckpt'\n",
        "        self.qg_model = QGModel.load_from_checkpoint(checkpoint_path)\n",
        "        self.qg_model.freeze()\n",
        "        self.qg_model.eval()\n",
        "\n",
        "    def generate(self, answer: str, context: str) -> str:\n",
        "        model_output = self._model_predict(answer, context)\n",
        "\n",
        "        generated_answer, generated_question = model_output.split('<sep>')\n",
        "\n",
        "        return generated_question\n",
        "\n",
        "    def generate_qna(self, context: str) -> Tuple[str, str]:\n",
        "        answer_mask = '[MASK]'\n",
        "        model_output = self._model_predict(answer_mask, context)\n",
        "\n",
        "        qna_pair = model_output.split('<sep>')\n",
        "\n",
        "        if len(qna_pair) < 2:\n",
        "            generated_answer = ''\n",
        "            generated_question = qna_pair[0]\n",
        "        else:\n",
        "            generated_answer = qna_pair[0]\n",
        "            generated_question = qna_pair[1]\n",
        "\n",
        "        return generated_answer, generated_question\n",
        "\n",
        "    def _model_predict(self, answer: str, context: str) -> str:\n",
        "        source_encoding = self.tokenizer(\n",
        "            '{} {} {}'.format(answer, SEP_TOKEN, context),\n",
        "            max_length=SOURCE_MAX_TOKEN_LEN,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        ).to('cuda')\n",
        "\n",
        "        generated_ids = self.qg_model.model.generate(\n",
        "            input_ids=source_encoding['input_ids'],\n",
        "            attention_mask=source_encoding['attention_mask'],\n",
        "            num_beams=16,\n",
        "            max_length=TARGET_MAX_TOKEN_LEN,\n",
        "            repetition_penalty=2.5,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "        preds = {\n",
        "            self.tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for generated_id in generated_ids\n",
        "        }\n",
        "\n",
        "        return ''.join(preds)"
      ],
      "metadata": {
        "id": "EEj74MyJ_qwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Answer Generation"
      ],
      "metadata": {
        "id": "7RFIb32HM-l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AnswerGenerator():\n",
        "    def __init__(self):\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "        checkpoint_path = 'best-checkpoint-v7.ckpt'\n",
        "        self.ag_model = QGModel.load_from_checkpoint(checkpoint_path)\n",
        "        self.ag_model.freeze()\n",
        "        self.ag_model.eval()\n",
        "\n",
        "    def generate(self, context: str, generate_count: int) -> List[str]:\n",
        "        model_output = self._model_predict(context, generate_count)\n",
        "\n",
        "        answers = model_output.replace('<pad>', '').split('</s>')[:-1]\n",
        "\n",
        "        return answers\n",
        "\n",
        "    def _model_predict(self, context: str, generate_count: int) -> str:\n",
        "        source_encoding = self.tokenizer(\n",
        "            context,\n",
        "            max_length=SOURCE_MAX_TOKEN_LEN,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        ).to('cuda')\n",
        "\n",
        "        generated_ids = self.ag_model.model.generate(\n",
        "            input_ids=source_encoding['input_ids'],\n",
        "            attention_mask=source_encoding['attention_mask'],\n",
        "            num_beams=generate_count,\n",
        "            num_return_sequences=generate_count,\n",
        "            max_length=TARGET_MAX_TOKEN_LEN,\n",
        "            repetition_penalty=2.5,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "        preds = {\n",
        "            self.tokenizer.decode(generated_id, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
        "            for generated_id in generated_ids\n",
        "        }\n",
        "\n",
        "        return ''.join(preds)"
      ],
      "metadata": {
        "id": "3qHkcCRVNBDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Distractor Generation"
      ],
      "metadata": {
        "id": "L3818V6238Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://drive.google.com/file/d/186jLSvapqLPtKsTrPKKFGwfQfd2N38C_/view?usp=sharing\n",
        "\n",
        "!gdown --id 186jLSvapqLPtKsTrPKKFGwfQfd2N38C_"
      ],
      "metadata": {
        "id": "AbRm_h-ZGQ9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistractorGenerator():\n",
        "    def __init__(self):\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "        # print('tokenizer len before: ', len(self.tokenizer))\n",
        "        self.tokenizer.add_tokens(SEP_TOKEN)\n",
        "        # print('tokenizer len after: ', len(self.tokenizer))\n",
        "        self.tokenizer_len = len(self.tokenizer)\n",
        "\n",
        "        checkpoint_path = 'best-checkpoint-v10.ckpt'\n",
        "        self.dg_model = QGModel.load_from_checkpoint(checkpoint_path)\n",
        "        self.dg_model.freeze()\n",
        "        self.dg_model.eval()\n",
        "\n",
        "    def generate(self, generate_count: int, correct: str, question: str, context: str) -> List[str]:\n",
        "\n",
        "        generate_triples_count = int(generate_count / 3) + 1 #since this model generates 3 distractors per generation\n",
        "\n",
        "        model_output = self._model_predict(generate_triples_count, correct, question, context)\n",
        "\n",
        "        cleaned_result = model_output.replace('<pad>', '').replace('</s>', '<sep>')\n",
        "        cleaned_result = self._replace_all_extra_id(cleaned_result)\n",
        "        distractors = cleaned_result.split('<sep>')[:-1]\n",
        "        distractors = [x.translate(str.maketrans('', '', string.punctuation)) for x in distractors]\n",
        "        distractors = list(map(lambda x: x.strip(), distractors))\n",
        "\n",
        "        return distractors\n",
        "\n",
        "    def _model_predict(self, generate_count: int, correct: str, question: str, context: str) -> str:\n",
        "        source_encoding = self.tokenizer(\n",
        "            '{} {} {} {} {}'.format(correct, SEP_TOKEN, question, SEP_TOKEN, context),\n",
        "            max_length= SOURCE_MAX_TOKEN_LEN,\n",
        "            padding='max_length',\n",
        "            truncation= True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "            ).to('cuda')\n",
        "\n",
        "        generated_ids = self.dg_model.model.generate(\n",
        "            input_ids=source_encoding['input_ids'],\n",
        "            attention_mask=source_encoding['attention_mask'],\n",
        "            num_beams=generate_count,\n",
        "            num_return_sequences=generate_count,\n",
        "            max_length=TARGET_MAX_TOKEN_LEN,\n",
        "            repetition_penalty=2.5,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "        preds = {\n",
        "            self.tokenizer.decode(generated_id, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
        "            for generated_id in generated_ids\n",
        "        }\n",
        "\n",
        "        return ''.join(preds)\n",
        "\n",
        "    def _correct_index_of(self, text:str, substring: str, start_index: int = 0):\n",
        "        try:\n",
        "            index = text.index(substring, start_index)\n",
        "        except ValueError:\n",
        "            index = -1\n",
        "\n",
        "        return index\n",
        "\n",
        "    def _replace_all_extra_id(self, text: str):\n",
        "        new_text = text\n",
        "        start_index_of_extra_id = 0\n",
        "\n",
        "        while (self._correct_index_of(new_text, '<extra_id_') >= 0):\n",
        "            start_index_of_extra_id = self._correct_index_of(new_text, '<extra_id_', start_index_of_extra_id)\n",
        "            end_index_of_extra_id = self._correct_index_of(new_text, '>', start_index_of_extra_id)\n",
        "\n",
        "            new_text = new_text[:start_index_of_extra_id] + '<sep>' + new_text[end_index_of_extra_id + 1:]\n",
        "\n",
        "        return new_text"
      ],
      "metadata": {
        "id": "EAkc04ZJ3_FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sence2vec Generation\n"
      ],
      "metadata": {
        "id": "HZMm6JzSWcS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sense2VecDistractorGeneration():\n",
        "    def __init__(self):\n",
        "        self.s2v = Sense2Vec().from_disk('s2v_old')\n",
        "\n",
        "    def generate(self, answer: str, desired_count: int) -> List[str]:\n",
        "        distractors = []\n",
        "        answer = answer.lower()\n",
        "        answer = answer.replace(\" \", \"_\")\n",
        "\n",
        "        sense = self.s2v.get_best_sense(answer)\n",
        "\n",
        "        if not sense:\n",
        "            return []\n",
        "\n",
        "        most_similar = self.s2v.most_similar(sense, n=desired_count)\n",
        "\n",
        "        for phrase in most_similar:\n",
        "            normalized_phrase = phrase[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
        "\n",
        "            if normalized_phrase.lower() != answer: #TODO: compare the stem of the words (e.g. wrote, writing)\n",
        "                distractors.append(normalized_phrase.capitalize())\n",
        "\n",
        "        return list(OrderedDict.fromkeys(distractors))"
      ],
      "metadata": {
        "id": "_uYKfn_oWbUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Questions"
      ],
      "metadata": {
        "id": "p8b-c01cJ9Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Question:\n",
        "    def __init__(self, answerText:str, questionText: str = '', distractors: List[str] = []):\n",
        "        self.answerText = answerText\n",
        "        self.questionText = questionText\n",
        "        self.distractors = distractors"
      ],
      "metadata": {
        "id": "KQEsWoaQKBM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MCQ Generation"
      ],
      "metadata": {
        "id": "DCYN28fDILbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MCQGenerator():\n",
        "    def __init__(self, is_verbose=False):\n",
        "        start_time = time.perf_counter()\n",
        "        print('Loading ML Models...')\n",
        "\n",
        "        # Currently not used\n",
        "        self.answer_generator = AnswerGenerator()\n",
        "        self.answer_generator.ag_model.to('cuda:0')\n",
        "        print('Loaded AnswerGenerator in', round(time.perf_counter() - start_time, 2), 'seconds.') if is_verbose else ''\n",
        "\n",
        "        self.question_generator = QuestionGenerator()\n",
        "        self.question_generator.qg_model.to('cuda:0')\n",
        "        print('Loaded QuestionGenerator in', round(time.perf_counter() - start_time, 2), 'seconds.') if is_verbose else ''\n",
        "\n",
        "        self.distractor_generator = DistractorGenerator()\n",
        "        self.distractor_generator.dg_model.to('cuda:0')\n",
        "        print('Loaded DistractorGenerator in', round(time.perf_counter() - start_time, 2), 'seconds.') if is_verbose else ''\n",
        "\n",
        "        self.sense2vec_distractor_generator = Sense2VecDistractorGeneration()\n",
        "        print('Loaded Sense2VecDistractorGenerator in', round(time.perf_counter() - start_time, 2), 'seconds.') if is_verbose else ''\n",
        "\n",
        "    # Main function\n",
        "\n",
        "    def generate_mcq_questions(self, dataset: pd.DataFrame, desired_total_count: int) -> List[Question]:\n",
        "        topics = dataset['topic'].unique()\n",
        "\n",
        "        questions = []\n",
        "\n",
        "        for topic in topics:\n",
        "            topic_dataset = dataset[dataset['topic'] == topic]\n",
        "            # Generate questions for the current topic\n",
        "            topic_questions = self._generate_question_answer_pairs(topic_dataset['context'].iloc[0], desired_count)\n",
        "            topic_questions = self._generate_distractors(topic_dataset['context'].iloc[0], topic_questions)\n",
        "            questions.extend(topic_questions)\n",
        "\n",
        "\n",
        "        # Display the generated questions\n",
        "            for i, question in enumerate(questions, 1):\n",
        "                print(f\"{i}: {question.questionText}\")\n",
        "                print(f\"Answer: {question.answerText}\")\n",
        "\n",
        "                print(\"Distractors:\")\n",
        "                for distractor in question.distractors:\n",
        "                    print(distractor)\n",
        "\n",
        "                print(\"\\n\")  # Add a new line between questions\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _generate_answers(self, context: str, desired_count: int) -> List[Question]:\n",
        "        answers = self.answer_generator.generate(context, desired_count)\n",
        "        answers = self._generate_multiple_answers_according_to_desired_count(context, desired_count)\n",
        "\n",
        "        print(answers)\n",
        "        unique_answers = remove_duplicates(answers)\n",
        "\n",
        "        questions = []\n",
        "        for answer in unique_answers:\n",
        "            questions.append(Question(answer))\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _generate_questions(self, context: str, questions: List[Question]) -> List[Question]:\n",
        "        for question in questions:\n",
        "            question.questionText = self.question_generator.generate(question.answerText, context)\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _generate_question_answer_pairs(self, context: str, desired_count: int) -> List[Question]:\n",
        "        context_splits = self._split_context_according_to_desired_count(context, desired_count)\n",
        "\n",
        "        questions = []\n",
        "\n",
        "        for split in context_splits:\n",
        "            answer, question = self.question_generator.generate_qna(split)\n",
        "            questions.append(Question(answer.capitalize(), question))\n",
        "\n",
        "        questions = list(toolz.unique(questions, key=lambda x: x.answerText))\n",
        "\n",
        "        return questions\n",
        "\n",
        "    def _generate_distractors(self, context: str, questions: List[Question]) -> List[Question]:\n",
        "        for question in questions:\n",
        "            t5_distractors = self.distractor_generator.generate(5, question.answerText, question.questionText, context)\n",
        "            # distractors = t5_distractors\n",
        "            if len(t5_distractors) < 3:\n",
        "                s2v_distractors = self.sense2vec_distractor_generator.generate(question.answerText, 3)\n",
        "                distractors = t5_distractors + s2v_distractors\n",
        "            else:\n",
        "                distractors = t5_distractors\n",
        "\n",
        "            distractors = remove_duplicates(distractors)\n",
        "            distractors = remove_distractors_duplicate_with_correct_answer(question.answerText, distractors)\n",
        "\n",
        "            question.distractors = distractors\n",
        "\n",
        "        return questions\n",
        "\n",
        "\n",
        "    # Helper functions\n",
        "    def _generate_answer_for_each_sentence(self, context: str) -> List[str]:\n",
        "        sents = sent_tokenize(context)\n",
        "\n",
        "        answers = []\n",
        "        for sent in sents:\n",
        "            answers.append(self.answer_generator.generate(sent, 1)[0])\n",
        "\n",
        "        return answers\n",
        "\n",
        "    #TODO: refactor to create better splits closer to the desired amount\n",
        "    def _split_context_according_to_desired_count(self, context: str, desired_count: int) -> List[str]:\n",
        "        if desired_count == 0:\n",
        "            return [context]\n",
        "\n",
        "        sents = sent_tokenize(context)\n",
        "        sent_ratio = len(sents) / desired_count\n",
        "\n",
        "        context_splits = []\n",
        "\n",
        "        if sent_ratio < 1:\n",
        "            return sents\n",
        "        else:\n",
        "            take_sents_count = int(sent_ratio + 1)\n",
        "\n",
        "            start_sent_index = 0\n",
        "\n",
        "            while start_sent_index < len(sents):\n",
        "                context_split = ' '.join(sents[start_sent_index: start_sent_index + take_sents_count])\n",
        "                context_splits.append(context_split)\n",
        "                start_sent_index += take_sents_count - 1\n",
        "\n",
        "        return context_splits\n",
        "\n"
      ],
      "metadata": {
        "id": "YhJv1wElIGo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "D7rWnG2DW2bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    # Move the MCQ_Generator to GPU\n",
        "    MCQ_Generator = MCQGenerator(True)\n",
        "\n",
        "    dataset = pd.read_csv('train.csv')\n",
        "    MCQ_Generator.generate_mcq_questions(dataset, 50)\n",
        "else:\n",
        "    print(\"GPU not available. Please ensure that your system has a compatible GPU.\")"
      ],
      "metadata": {
        "id": "IM-Fqan8GVzr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}