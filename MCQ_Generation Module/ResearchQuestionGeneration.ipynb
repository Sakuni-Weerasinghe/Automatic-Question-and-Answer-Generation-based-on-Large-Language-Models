{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakuni-Weerasinghe/Automatic-Question-and-Answer-Generation-based-on-Large-Language-Models/blob/master/ResearchQuestionGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "!pip install sentencepiece\n",
        "!pip install pytorch-lightning\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "6SfEPa1vRv7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTGJqWL17k5w"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RH4NbvQ7nyV"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "import tqdm.notebook as tq\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from transformers import T5Tokenizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning import Trainer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast as T5Tokenizer\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmmqT0VG713d"
      },
      "outputs": [],
      "source": [
        "def extract_from_my_dataset(data):\n",
        "\n",
        "    topics = []\n",
        "    sub_topics = []\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    correct_answers = []\n",
        "    options1 = []  # Renamed option1\n",
        "    options2 = []  # Renamed option2\n",
        "    options3 = []  # Renamed option3\n",
        "    options4 = []  # Renamed option4\n",
        "\n",
        "    for topic in data:\n",
        "        topic_name = topic.get(\"topic\", \"\")\n",
        "        sub_topic_name = topic.get(\"sub-topic\", \"\")\n",
        "        context = topic.get(\"context\", \"\")\n",
        "        topic_questions = topic.get(\"questions\", [])\n",
        "\n",
        "        for qna_set in topic_questions:\n",
        "            question = qna_set.get(\"question\", \"\")\n",
        "            correct_answer = qna_set.get(\"correct_answer\", \"\")\n",
        "            option_1 = qna_set.get(\"option1\", \"\")  # Renamed option1\n",
        "            option_2 = qna_set.get(\"option2\", \"\")  # Renamed option2\n",
        "            option_3 = qna_set.get(\"option3\", \"\")  # Renamed option3\n",
        "            option_4 = qna_set.get(\"option4\", \"\")  # Renamed option4\n",
        "\n",
        "            topics.append(topic_name)\n",
        "            sub_topics.append(sub_topic_name)\n",
        "            contexts.append(context)\n",
        "            questions.append(question)\n",
        "            correct_answers.append(correct_answer)\n",
        "            options1.append(option_1)  # Appended to options1\n",
        "            options2.append(option_2)  # Appended to options2\n",
        "            options3.append(option_3)  # Appended to options3\n",
        "            options4.append(option_4)  # Appended to options4\n",
        "\n",
        "    return topics, sub_topics, contexts, questions, correct_answers, options1, options2, options3, options4\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_json(filepath):\n",
        "    data = []\n",
        "\n",
        "    with open(filepath) as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "8yHSlCXeSKoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = parse_json(\"data.json\")  # Replace with your actual dataset\n",
        "\n",
        "topics, sub_topics, contexts, questions, correct_answers, option1,option2,option3,option4 = extract_from_my_dataset(data)\n",
        "\n",
        "# Create a DataFrame based on extracted data\n",
        "my_dataset_df = pd.DataFrame({\n",
        "    'topic': topics,\n",
        "    'sub_topic': sub_topics,\n",
        "    'context': contexts,\n",
        "    'question': questions,\n",
        "    'correct_answer': correct_answers,\n",
        "    'option1': option1,\n",
        "    'option2': option2,\n",
        "    'option3': option3,\n",
        "    'option4': option4\n",
        "})"
      ],
      "metadata": {
        "id": "gKnCNgtLR23C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtlU284K0rWN"
      },
      "source": [
        "Export as *.csv and upload to GDrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "id": "GxcVRO5eShV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train, test, and validation sets\n",
        "train, test = train_test_split(my_dataset_df, test_size=0.2, random_state=42)\n",
        "train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(val.shape)"
      ],
      "metadata": {
        "id": "nVvKs4OIZpS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDWtj9Hm0xWS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train.to_csv(\"train.csv\", index=False)\n",
        "test.to_csv(\"test.csv\", index=False)\n",
        "val.to_csv(\"val.csv\", index=False)\n",
        "\n",
        "!mv train.csv test.csv val.csv drive/MyDrive/Research_Final/QG/DataSet\n",
        "\n",
        "!gdown 'https://drive.google.com/file/d/1-XbPy7_ooHU1Wg8-RDWXw7ZGiq60kUSP' -O train.csv\n",
        "!gdown 'https://drive.google.com/file/d/1-SRGIQoJ4fYh1iX9Suc5RmOJgCgny1y3' -O test.csv\n",
        "!gdown 'https://drive.google.com/file/d/1-OexDBlKaRWIZac2lKqGxIwnZi0UdcDk' -O val.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJKsPKJ6zuVG"
      },
      "outputs": [],
      "source": [
        "train_dataset = pd.read_csv('train.csv')\n",
        "test_dataset = pd.read_csv('test.csv')\n",
        "val_dataset = pd.read_csv('val.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_1 = train.copy()\n",
        "test_dataset_1 = test.copy()\n",
        "val_dataset_1 = val.copy()\n",
        "\n",
        "train_dataset_1 = train_dataset_1.dropna() #removing rows with missing values in the DataFrame\n",
        "test_dataset_1 = test_dataset_1.dropna() #removing rows with missing values in the DataFrame\n",
        "val_dataset_1 = val_dataset_1.dropna() #removing rows with missing values in the DataFrame\n",
        "\n",
        "train_dataset_1.drop(columns=['option1', 'option2','option3','option4'], inplace=True)\n",
        "test_dataset_1.drop(columns=['option1', 'option2','option3','option4'], inplace=True)\n",
        "val_dataset_1.drop(columns=['option1', 'option2','option3','option4'], inplace=True)\n",
        "\n",
        "print(train_dataset_1.shape,'train_dataset_1')\n",
        "print(test_dataset_1.shape, 'test_dataset_1')\n",
        "print(val_dataset_1.shape, 'val_dataset_1')\n"
      ],
      "metadata": {
        "id": "wpGeBXqbZ2qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVSF1Zzir-Cp"
      },
      "source": [
        "**Pytorch Lightning Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8_R_ru2F5CA"
      },
      "outputs": [],
      "source": [
        "SEP_TOKEN = '<sep>'\n",
        "MASKING_CHANCE = 0.3\n",
        "#30% chance to replace the answer with '[MASK]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BqPpNABF54P"
      },
      "outputs": [],
      "source": [
        "class QGDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        tokenizer: T5Tokenizer,\n",
        "        source_max_token_len: int,\n",
        "        target_max_token_len: int,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.source_max_token_len = source_max_token_len\n",
        "        self.target_max_token_len = target_max_token_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        data_row = self.data.iloc[index]  # Use iloc to access the row by index\n",
        "\n",
        "        if np.random.rand() > MASKING_CHANCE:\n",
        "            answer = data_row['correct_answer']\n",
        "        else:\n",
        "            answer = '[MASK]'\n",
        "\n",
        "        source_encoding = self.tokenizer(\n",
        "            '{} {} {}'.format(answer, SEP_TOKEN, data_row['context']),\n",
        "            max_length=self.source_max_token_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        target_encoding = self.tokenizer(\n",
        "            '{} {} {}'.format(data_row['correct_answer'], SEP_TOKEN, data_row['question']),\n",
        "            max_length=self.target_max_token_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        labels = target_encoding['input_ids']\n",
        "        labels[labels == 0] = -100\n",
        "\n",
        "        return dict(\n",
        "            answer_text=data_row['correct_answer'],\n",
        "            context=data_row['context'],\n",
        "            question=data_row['question'],\n",
        "            input_ids=source_encoding['input_ids'].flatten(),\n",
        "            attention_mask=source_encoding['attention_mask'].flatten(),\n",
        "            labels=labels.flatten().to(torch.long)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dZLwDcQsEKa"
      },
      "source": [
        "**Pytorch Lightning DataModule**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLYFCLppGdL3"
      },
      "outputs": [],
      "source": [
        "class QGDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_dataset_1: pd.DataFrame,\n",
        "        val_dataset_1: pd.DataFrame,\n",
        "        test_dataset_1: pd.DataFrame,\n",
        "        tokenizer: T5Tokenizer,\n",
        "        batch_size,\n",
        "        source_max_token_len: int,\n",
        "        target_max_token_len: int\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_dataset_1 = train_dataset_1\n",
        "        self.val_dataset_1 = val_dataset_1\n",
        "        self.test_dataset_1 = test_dataset_1\n",
        "        self.tokenizer = tokenizer\n",
        "        self.source_max_token_len = source_max_token_len\n",
        "        self.target_max_token_len = target_max_token_len\n",
        "\n",
        "    def setup(self,stage=None):\n",
        "        self.train_dataset_2 = QGDataset(self.train_dataset_1, self.tokenizer, self.source_max_token_len, self.target_max_token_len)\n",
        "        self.val_dataset_2 = QGDataset(self.val_dataset_1, self.tokenizer, self.source_max_token_len, self.target_max_token_len)\n",
        "        self.test_dataset_2 = QGDataset(self.test_dataset_1, self.tokenizer, self.source_max_token_len, self.target_max_token_len)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset_2, batch_size = self.batch_size, shuffle=True, num_workers = 2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset_2, batch_size=1, num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset_2, batch_size=1, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P58yvTeisH44"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvt4Np0AGhzc"
      },
      "outputs": [],
      "source": [
        "#use t5-base, gpt-2 as other models\n",
        "MODEL_NAME = 't5-small'\n",
        "SOURCE_MAX_TOKEN_LEN = 1000\n",
        "TARGET_MAX_TOKEN_LEN = 80\n",
        "\n",
        "N_EPOCHS = 20\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DF_TAKE_PERCENTAGE = 1\n",
        "\n",
        "TAKE_TRAIN = int(len(train_dataset_1) * DF_TAKE_PERCENTAGE)\n",
        "TAKE_TEST = int(len(test_dataset_1) * DF_TAKE_PERCENTAGE)\n",
        "TAKE_VAL = int(len(val_dataset_1) * DF_TAKE_PERCENTAGE)"
      ],
      "metadata": {
        "id": "4GDKMXltSu_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI6P0wC9sSIu"
      },
      "source": [
        "**Initializing training module**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl5wH8MNsaQH"
      },
      "source": [
        "Setting Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "print('tokenizer len before: ', len(tokenizer))\n",
        "tokenizer.add_tokens(SEP_TOKEN)\n",
        "print('tokenizer len after: ', len(tokenizer))\n",
        "TOKENIZER_LEN = len(tokenizer)\n",
        "\n",
        "train_dataset_3 = train_dataset_1[:TAKE_TRAIN]\n",
        "test_dataset_3 = test_dataset_1[:TAKE_TEST]\n",
        "val_dataset_3 = val_dataset_1[:TAKE_VAL]\n",
        "\n",
        "\n",
        "data_module = QGDataModule(\n",
        "    train_dataset_3,\n",
        "    test_dataset_3,\n",
        "    val_dataset_3,\n",
        "    tokenizer,\n",
        "    BATCH_SIZE,\n",
        "    SOURCE_MAX_TOKEN_LEN,\n",
        "    TARGET_MAX_TOKEN_LEN)\n",
        "\n",
        "data_module.setup()\n",
        "\n",
        "train_dataloader = data_module.train_dataloader()\n",
        "val_dataloader = data_module.val_dataloader()\n",
        "test_dataloader = data_module.test_dataloader()"
      ],
      "metadata": {
        "id": "P-KpIsRfSy9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtpNX_JYsbHf"
      },
      "outputs": [],
      "source": [
        "class QGModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
        "        self.model.resize_token_embeddings(TOKENIZER_LEN) #resizing after adding new tokens to the tokenizer\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "        self.log('train_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "        self.log('val_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        loss, output = self(input_ids, attention_mask, labels)\n",
        "        self.log('test_loss', loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return AdamW(self.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYNPlLN4tNgP"
      },
      "source": [
        "Setting trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='checkpoints',\n",
        "    filename='best-checkpoint',\n",
        "    save_top_k=-1,\n",
        "    verbose=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min'\n",
        "    )\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    callbacks=[checkpoint_callback],\n",
        "    max_epochs=N_EPOCHS,\n",
        "    devices=1\n",
        "    )"
      ],
      "metadata": {
        "id": "xN2F2eKgTGl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFI8qO47z8mf"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC0slHBlz96x"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = QGModel.load_from_checkpoint('checkpoints/best-checkpoint-v42.ckpt')\n",
        "model = QGModel()\n",
        "trainer.fit(model, data_module)\n",
        "trainer.test(model, data_module)"
      ],
      "metadata": {
        "id": "eBAPJ-TMTMzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD8ezaLJ91gx"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkdmGsFt98pF",
        "outputId": "2eb8b1ab-c408-42a0-a9e7-26846be1bafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "checkpoint_path = 'best-checkpoint-v5.ckpt'\n",
        "\n",
        "# Check if the file exists before attempting to load\n",
        "if os.path.exists(checkpoint_path):\n",
        "    best_model = QGModel.load_from_checkpoint(checkpoint_path)\n",
        "    best_model.freeze()\n",
        "    best_model.eval()\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(f\"Checkpoint file '{checkpoint_path}' not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rldb_yv-EBB"
      },
      "source": [
        "### Common functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5YD5xjA-KYS"
      },
      "outputs": [],
      "source": [
        "def generate(qgmodel: QGModel, answer: str, context: str) -> str:\n",
        "    source_encoding = tokenizer(\n",
        "        '{} {} {}'.format(answer, SEP_TOKEN, context),\n",
        "        max_length=SOURCE_MAX_TOKEN_LEN,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors='pt'\n",
        "    ).to('cuda')\n",
        "\n",
        "    qgmodel.model.to('cuda')\n",
        "\n",
        "    generated_ids = qgmodel.model.generate(\n",
        "        input_ids=source_encoding['input_ids'].to('cuda'),\n",
        "        attention_mask=source_encoding['attention_mask'].to('cuda'),\n",
        "        num_beams=1,\n",
        "        max_length=TARGET_MAX_TOKEN_LEN,\n",
        "        repetition_penalty=2.5,\n",
        "        length_penalty=1.0,\n",
        "        early_stopping=True,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    preds = {\n",
        "        tokenizer.decode(generated_id, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
        "        for generated_id in generated_ids\n",
        "    }\n",
        "\n",
        "    return ''.join(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZMKAH77-OPJ"
      },
      "outputs": [],
      "source": [
        "def show_result(generated: str, answer: str, topic:str,sub_topic:str):\n",
        "    print('Generated: ', generated)\n",
        "    print()\n",
        "    print('Answer: ', answer)\n",
        "    print('Conext: ', topic)\n",
        "    print('Conext: ', sub_topic)\n",
        "    print('-----------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKe6-zdZ-lC5"
      },
      "source": [
        "### View results manually"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_question = test_dataset_1.iloc[12]\n",
        "\n",
        "\n",
        "generated = generate(best_model, sample_question['correct_answer'], sample_question['context'])\n",
        "show_result(generated, sample_question['correct_answer'], sample_question['context'], sample_question['question'])"
      ],
      "metadata": {
        "id": "K4tcaaR5TuK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHh7MeOScJmU"
      },
      "source": [
        "#### Answer-aware question generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test_dataset_1[:10])):\n",
        "    context = test_dataset_1.iloc[i]['context']\n",
        "    answer = test_dataset_1.iloc[i]['correct_answer']\n",
        "\n",
        "    generated = generate(best_model, answer, context)\n",
        "\n",
        "    show_result(generated, answer, context, test_dataset_1.iloc[i]['question'])"
      ],
      "metadata": {
        "id": "OsvkLlcCTyya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6MdgwMPca2E"
      },
      "source": [
        "#### Generating both answer and question"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test_dataset_1[:10])):\n",
        "    context = test_dataset_1.iloc[i]['context']\n",
        "    original_answer = test_dataset_1.iloc[i]['correct_answer']\n",
        "    input_answer = '[MASK]'\n",
        "\n",
        "    generated = generate(best_model, input_answer, context)\n",
        "\n",
        "    show_result(generated, original_answer, context, test_dataset_1.iloc[i]['question'])"
      ],
      "metadata": {
        "id": "U0Mjgb_rT3TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "k_5GeEl3H0cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(reference, candidate):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    return sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method4)\n",
        "\n",
        "def calculate_rouge_n(reference, generated, n=1):\n",
        "    reference_tokens = word_tokenize(reference)\n",
        "    generated_tokens = word_tokenize(generated)\n",
        "\n",
        "    reference_ngrams = list(ngrams(reference_tokens, n))\n",
        "    generated_ngrams = list(ngrams(generated_tokens, n))\n",
        "\n",
        "    # Calculate overlap (common n-grams)\n",
        "    overlapping_ngrams = set(reference_ngrams) & set(generated_ngrams)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision = len(overlapping_ngrams) / len(generated_ngrams) if len(generated_ngrams) > 0 else 0\n",
        "    recall = len(overlapping_ngrams) / len(reference_ngrams) if len(reference_ngrams) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1_score\n",
        "\n"
      ],
      "metadata": {
        "id": "c3kVD1HFH5Ar"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOysBwYqd2fOGFUSEauicsC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}